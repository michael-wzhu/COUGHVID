{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/parulnith/7f8c174e6ac099e86f0495d3d9a4c01e/untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNnM2w-HCeb1"
   },
   "source": [
    "# Cough classification notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 ffmpeg\n",
    "\n",
    "# apt update\n",
    "# apt install ffmpeg\n",
    "# ffmpeg -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l3sppZMCydR"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gt3fyg6dCNvX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\github_projects\\cough_videos\n"
     ]
    }
   ],
   "source": [
    "# feature extractoring and preprocessing data\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# #Keras\n",
    "# import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%cd ../../\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPe_ebYuDqr5"
   },
   "source": [
    "## Extracting cough audios and features\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use [COUGHVID](https://zenodo.org/record/7024894#.YysPHHZBybg) dataset for classification. \n",
    "<br>\n",
    "<br>\n",
    "数据原本将每段咳嗽音区分为 i.e\n",
    " * COVID-19\n",
    " * healthy\n",
    " * symptomatic\n",
    " \n",
    "Total dataset: xxx samples\n",
    "\n",
    "\n",
    "我们现在将任务改为二分类：healthy (label 0) vs unhealthy (label 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "neqMS0VoDpN5"
   },
   "source": [
    "## Meta data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 查看每个json文件，选取标注\n",
    "\n",
    "# import json\n",
    "\n",
    "# dict_id2label = {}\n",
    "\n",
    "# folder = './datasets/coughvid_v1/public_dataset'\n",
    "# count = 0\n",
    "# for filename in os.listdir(folder):\n",
    "# #     print(filename)\n",
    "#     if not filename.endswith(\"json\"):\n",
    "#         continue\n",
    "    \n",
    "#     annotation_info = json.load(\n",
    "#         open(os.path.join(folder, filename), \"r\", encoding=\"utf-8\")\n",
    "#     )\n",
    "# #     print(annotation_info)\n",
    "    \n",
    "#     expert_anno_1 = annotation_info.get(\"expert_labels_1\")\n",
    "#     diagnosis_1 = None\n",
    "#     if expert_anno_1:\n",
    "#         diagnosis_1 = expert_anno_1[\"diagnosis\"]\n",
    "#         if expert_anno_1[\"quality\"] == \"no_cough\":\n",
    "#             continue\n",
    "    \n",
    "#     expert_anno_2 = annotation_info.get(\"expert_labels_2\")\n",
    "#     diagnosis_2 = None\n",
    "#     if expert_anno_2:\n",
    "#         diagnosis_2 = expert_anno_2[\"diagnosis\"]\n",
    "#         if expert_anno_2[\"quality\"] == \"no_cough\":\n",
    "#             continue\n",
    "    \n",
    "#     expert_anno_3 = annotation_info.get(\"expert_labels_3\")\n",
    "#     diagnosis_3 = None\n",
    "#     if expert_anno_3:\n",
    "#         diagnosis_3 = expert_anno_3.get(\"diagnosis\")\n",
    "# #         diagnosis_3 = expert_anno_3[\"diagnosis\"]\n",
    "#         if expert_anno_3[\"quality\"] == \"no_cough\":\n",
    "#             continue\n",
    "    \n",
    "#     if not diagnosis_1 and not diagnosis_2 and not diagnosis_3:\n",
    "#         continue\n",
    "    \n",
    "#     status = annotation_info.get(\"status\")\n",
    "    \n",
    "#     diagnose_label = None\n",
    "#     if set([diagnosis_1, diagnosis_2, diagnosis_3]) == set([\"healthy_cough\", None]):\n",
    "#         diagnose_label = \"healthy\"\n",
    "#     elif set([diagnosis_1, diagnosis_2, diagnosis_3]) == set([\"healthy_cough\"]):\n",
    "#         diagnose_label = \"healthy\"\n",
    "#     elif \"healthy\" not in json.dumps([diagnosis_1, diagnosis_2, diagnosis_3]):\n",
    "#         diagnose_label = \"unhealthy\"\n",
    "#     else:\n",
    "#         if status:\n",
    "#             if status == \"healthy\":\n",
    "#                 diagnose_label = \"healthy\"\n",
    "#             else:\n",
    "#                 diagnose_label = \"unhealthy\"\n",
    "        \n",
    "#         else:        \n",
    "#             print(json.dumps(sorted([diagnosis_1, diagnosis_2, diagnosis_3])))\n",
    "#             if \"\\\"healthy_cough\\\", \\\"healthy_cough\\\"\" not in json.dumps(sorted([diagnosis_1, diagnosis_2, diagnosis_3])):\n",
    "#                 diagnose_label = \"unhealthy\"\n",
    "#             else:\n",
    "#                 diagnose_label = \"healthy\"\n",
    "    \n",
    "#     count += 1\n",
    "#     dict_id2label[filename.replace(\".json\", \"\")] = 1 if diagnose_label == \"unhealthy\" else 0\n",
    "\n",
    "# print(\"有效样本： \", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 正负样本数量：\n",
    "# neg_sample = 0   # healthy coughs\n",
    "# for k, v in dict_id2label.items():\n",
    "#     if v == 0:\n",
    "#         neg_sample += 1\n",
    "\n",
    "# print(\"负样本率： \", neg_sample / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piwUwgP5Eef9"
   },
   "source": [
    "## Extracting features from Spectrogram\n",
    "\n",
    "\n",
    "We will extract\n",
    "\n",
    "* Mel-frequency cepstral coefficients (MFCC)(20 in number)\n",
    "* Spectral Centroid,\n",
    "* Zero Crossing Rate\n",
    "* Chroma Frequencies\n",
    "* Spectral Roll-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__g8tX8pDeIL"
   },
   "outputs": [],
   "source": [
    "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}_mean'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}_std'\n",
    "header += ' label'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBlT448pEqR9"
   },
   "source": [
    "## Writing data to csv file\n",
    "\n",
    "We write the data to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsSQmB0PE3Iu"
   },
   "outputs": [],
   "source": [
    "# f = open('./datasets/coughvid_v1/features/data.csv', 'w', encoding=\"utf-8\")\n",
    "# f.write(header + \"\\n\")\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# for filename in tqdm(os.listdir(folder)):\n",
    "    \n",
    "#     if filename.endswith(\"json\"):\n",
    "#         continue\n",
    "#     if filename.endswith(\"csv\"):\n",
    "#         continue\n",
    "    \n",
    "#     idx = filename.split(\".\")[0]\n",
    "#     if idx not in dict_id2label:\n",
    "#         continue\n",
    "    \n",
    "#     label = dict_id2label[idx]\n",
    "    \n",
    "#     # print(label, idx)\n",
    "    \n",
    "#     # load file\n",
    "#     y, sr = librosa.load(os.path.join(folder, filename), mono=True, duration=30)\n",
    "#     # print(y.shape)\n",
    "#     # print(sr)\n",
    "    \n",
    "#     chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "#     rmse = librosa.feature.rms(y=y, )\n",
    "#     spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "#     spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "#     rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "#     zcr = librosa.feature.zero_crossing_rate(y)\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "#     to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "\n",
    "#     for e in mfcc:\n",
    "#         to_append += f' {np.mean(e)}'\n",
    "#     for e in mfcc:\n",
    "#         to_append += f' {np.std(e)}'\n",
    "            \n",
    "#     to_append += f' {label}'\n",
    "#     # print(to_append)\n",
    "#     f.write(to_append + \"\\n\")\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yfdo1cj6V7d"
   },
   "source": [
    "The data has been extracted into a [data.csv](https://github.com/parulnith/Music-Genre-Classification-with-Python/blob/master/data.csv) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgeCZSKQEp1A"
   },
   "source": [
    "# Analysing the Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "Kr5_EdpD9dyh",
    "outputId": "81fd4a29-93fa-44f8-bf90-2f99981f761a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>rmse</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1_mean</th>\n",
       "      <th>mfcc2_mean</th>\n",
       "      <th>mfcc3_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12_std</th>\n",
       "      <th>mfcc13_std</th>\n",
       "      <th>mfcc14_std</th>\n",
       "      <th>mfcc15_std</th>\n",
       "      <th>mfcc16_std</th>\n",
       "      <th>mfcc17_std</th>\n",
       "      <th>mfcc18_std</th>\n",
       "      <th>mfcc19_std</th>\n",
       "      <th>mfcc20_std</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22dcfcdd-52a8-4b42-9c50-c0a3696382a9.webm</td>\n",
       "      <td>0.187894</td>\n",
       "      <td>0.025356</td>\n",
       "      <td>963.070505</td>\n",
       "      <td>846.074789</td>\n",
       "      <td>1932.604980</td>\n",
       "      <td>0.059711</td>\n",
       "      <td>-472.439301</td>\n",
       "      <td>16.604473</td>\n",
       "      <td>-7.377993</td>\n",
       "      <td>...</td>\n",
       "      <td>11.093034</td>\n",
       "      <td>8.206923</td>\n",
       "      <td>8.646673</td>\n",
       "      <td>6.249838</td>\n",
       "      <td>3.590692</td>\n",
       "      <td>3.762098</td>\n",
       "      <td>9.720296</td>\n",
       "      <td>3.636496</td>\n",
       "      <td>4.808792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d50fb043-08aa-4960-94f7-e9bbc690e70d.webm</td>\n",
       "      <td>0.042411</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>234.919076</td>\n",
       "      <td>205.140347</td>\n",
       "      <td>446.992558</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>-562.550110</td>\n",
       "      <td>12.425411</td>\n",
       "      <td>-3.398605</td>\n",
       "      <td>...</td>\n",
       "      <td>7.752041</td>\n",
       "      <td>5.287385</td>\n",
       "      <td>4.998383</td>\n",
       "      <td>4.661725</td>\n",
       "      <td>7.737397</td>\n",
       "      <td>5.847256</td>\n",
       "      <td>6.207181</td>\n",
       "      <td>5.978804</td>\n",
       "      <td>2.865030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f5f4027-45d4-4867-a9e2-ecc500eaf21d.webm</td>\n",
       "      <td>0.557196</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>2785.087057</td>\n",
       "      <td>2211.925156</td>\n",
       "      <td>5268.870618</td>\n",
       "      <td>0.191680</td>\n",
       "      <td>-505.479309</td>\n",
       "      <td>31.321623</td>\n",
       "      <td>-5.810165</td>\n",
       "      <td>...</td>\n",
       "      <td>7.691325</td>\n",
       "      <td>6.780107</td>\n",
       "      <td>4.998759</td>\n",
       "      <td>4.860253</td>\n",
       "      <td>7.856476</td>\n",
       "      <td>3.556230</td>\n",
       "      <td>4.408528</td>\n",
       "      <td>4.954839</td>\n",
       "      <td>8.589847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8a5feabb-3b0a-49b5-900c-6d3d31e96794.webm</td>\n",
       "      <td>0.427177</td>\n",
       "      <td>0.054161</td>\n",
       "      <td>2766.423560</td>\n",
       "      <td>2258.583563</td>\n",
       "      <td>5397.398633</td>\n",
       "      <td>0.235709</td>\n",
       "      <td>-333.071655</td>\n",
       "      <td>48.053001</td>\n",
       "      <td>-18.119431</td>\n",
       "      <td>...</td>\n",
       "      <td>7.634245</td>\n",
       "      <td>8.813775</td>\n",
       "      <td>7.188757</td>\n",
       "      <td>8.187786</td>\n",
       "      <td>8.326145</td>\n",
       "      <td>7.226190</td>\n",
       "      <td>7.231071</td>\n",
       "      <td>8.264647</td>\n",
       "      <td>6.015858</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>960336ac-2567-4136-8ee8-f8564d556822.webm</td>\n",
       "      <td>0.421020</td>\n",
       "      <td>0.031633</td>\n",
       "      <td>2478.157650</td>\n",
       "      <td>1981.974390</td>\n",
       "      <td>4688.709486</td>\n",
       "      <td>0.219031</td>\n",
       "      <td>-455.479889</td>\n",
       "      <td>35.985081</td>\n",
       "      <td>-29.793978</td>\n",
       "      <td>...</td>\n",
       "      <td>13.291973</td>\n",
       "      <td>9.249558</td>\n",
       "      <td>8.176953</td>\n",
       "      <td>6.090113</td>\n",
       "      <td>7.761150</td>\n",
       "      <td>6.474304</td>\n",
       "      <td>5.309581</td>\n",
       "      <td>5.311271</td>\n",
       "      <td>8.227166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename  chroma_stft      rmse  \\\n",
       "0  22dcfcdd-52a8-4b42-9c50-c0a3696382a9.webm     0.187894  0.025356   \n",
       "1  d50fb043-08aa-4960-94f7-e9bbc690e70d.webm     0.042411  0.006042   \n",
       "2  5f5f4027-45d4-4867-a9e2-ecc500eaf21d.webm     0.557196  0.015863   \n",
       "3  8a5feabb-3b0a-49b5-900c-6d3d31e96794.webm     0.427177  0.054161   \n",
       "4  960336ac-2567-4136-8ee8-f8564d556822.webm     0.421020  0.031633   \n",
       "\n",
       "   spectral_centroid  spectral_bandwidth      rolloff  zero_crossing_rate  \\\n",
       "0         963.070505          846.074789  1932.604980            0.059711   \n",
       "1         234.919076          205.140347   446.992558            0.015784   \n",
       "2        2785.087057         2211.925156  5268.870618            0.191680   \n",
       "3        2766.423560         2258.583563  5397.398633            0.235709   \n",
       "4        2478.157650         1981.974390  4688.709486            0.219031   \n",
       "\n",
       "   mfcc1_mean  mfcc2_mean  mfcc3_mean  ...  mfcc12_std  mfcc13_std  \\\n",
       "0 -472.439301   16.604473   -7.377993  ...   11.093034    8.206923   \n",
       "1 -562.550110   12.425411   -3.398605  ...    7.752041    5.287385   \n",
       "2 -505.479309   31.321623   -5.810165  ...    7.691325    6.780107   \n",
       "3 -333.071655   48.053001  -18.119431  ...    7.634245    8.813775   \n",
       "4 -455.479889   35.985081  -29.793978  ...   13.291973    9.249558   \n",
       "\n",
       "   mfcc14_std  mfcc15_std  mfcc16_std  mfcc17_std  mfcc18_std  mfcc19_std  \\\n",
       "0    8.646673    6.249838    3.590692    3.762098    9.720296    3.636496   \n",
       "1    4.998383    4.661725    7.737397    5.847256    6.207181    5.978804   \n",
       "2    4.998759    4.860253    7.856476    3.556230    4.408528    4.954839   \n",
       "3    7.188757    8.187786    8.326145    7.226190    7.231071    8.264647   \n",
       "4    8.176953    6.090113    7.761150    6.474304    5.309581    5.311271   \n",
       "\n",
       "   mfcc20_std  label  \n",
       "0    4.808792      1  \n",
       "1    2.865030      1  \n",
       "2    8.589847      1  \n",
       "3    6.015858      1  \n",
       "4    8.227166      1  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./datasets/coughvid_v1/features/data.csv', sep=\" \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iHrDHCaR9gKR",
    "outputId": "7d32943a-1ad5-4a59-c13a-beebeb36e4c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2138, 48)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veD5BgX49hZa"
   },
   "outputs": [],
   "source": [
    "# Dropping unneccesary columns\n",
    "data = data.drop(['filename'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.baseline1_feature_extraction.tree_training_utils import train_cv_with_thres, threshold_prob, threshold_prob_via_jordan\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "random_state = 100\n",
    "cbt = CatBoostClassifier(verbose=0, allow_writing_files=False, random_state=random_state)\n",
    "lgb = LGBMClassifier(is_unbalance=True, random_state=random_state)\n",
    "xgb = XGBClassifier(verbose=0, scale_pos_weight=3, enable_categorical=True, random_state=random_state)\n",
    "logistic = LogisticRegression(verbose=0, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "list_model_names = [\n",
    "    \"cbt\",\n",
    "    \"lgb\",\n",
    "    \"xgb\",\n",
    "    \"logistic\",\n",
    "#     \"stack\"\n",
    "]\n",
    "list_model_clfs = [\n",
    "    cbt, \n",
    "    lgb, \n",
    "    xgb, \n",
    "    logistic, \n",
    "#     stack\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "features = [w for w in data.columns if w not in [\"label\"]]\n",
    "print(features)\n",
    "\n",
    "for clf_name, clf in zip(list_model_names, list_model_clfs):\n",
    "    \n",
    "    res = train_cv_with_thres(data,\n",
    "                        clf,\n",
    "                        features,\n",
    "                        num_folds=5,\n",
    "                        random_state=101,\n",
    "                        sensitivity_threshold=0.75,\n",
    "                        save_dir=None,\n",
    "                        )\n",
    "    print(f\"**{clf_name}**\" * 30)\n",
    "    print(res)\n",
    "    print(f\"**{clf_name}**\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyr0aAAsGXjZ"
   },
   "source": [
    "## Encoding the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frI5HH4q-1HS"
   },
   "outputs": [],
   "source": [
    "genre_list = data.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(genre_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Slm8W0-iGVhI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2n8a02zGfvP"
   },
   "source": [
    "## Scaling the Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqcqn-nyAofk"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3VZvbwpGo9R"
   },
   "source": [
    "## Dividing data into training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1GW3VvQA7Rj"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "upuczQ-KBHJ5",
    "outputId": "1431a28b-e8b6-4db2-e505-7e149e37c0d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1710"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LtoE_FqqBzM8",
    "outputId": "76555a2b-2030-48e1-b52d-d71b4ebae38e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ir9XaWgQB0lq",
    "outputId": "2ec90814-19d8-4f27-934a-1ce54406d4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3591301 , -0.25922497, -0.57788119,  0.06884817, -0.33089911,\n",
       "       -0.83752411,  0.32502235,  0.90580188, -0.13074752, -0.27530241,\n",
       "        0.62560535, -1.17995162,  1.26249642,  0.40421693,  0.3379524 ,\n",
       "        1.23840052,  1.66424156,  1.42644844,  0.73458758, -0.07949798,\n",
       "       -0.609472  ,  0.26316202, -0.32750201, -1.96656077, -1.47049683,\n",
       "       -0.28576964, -0.19126953,  1.66445639,  0.33872014, -0.94282109,\n",
       "       -1.3252734 , -0.98665131, -1.77422902, -1.48956221, -0.58558322,\n",
       "       -1.67459028, -1.42597955, -2.01056192, -1.48048068, -0.86154918,\n",
       "       -1.06210475, -1.12256077, -1.2109277 ,  0.58490895, -0.2545372 ,\n",
       "       -1.34971647])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cls with CatBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:11,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:16,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:22,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 1)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "(427, 1)\n",
      "(427, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:27,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 1)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "**cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt**\n",
      "{'敏感性': [0.752808988764045, 0.752808988764045, 0.752808988764045, 0.75, 0.752808988764045], '特异性': [0.41002949852507375, 0.35988200589970504, 0.46017699115044247, 0.35103244837758113, 0.4467455621301775], '敏感性_jordan': [0.5730337078651686, 0.4382022471910112, 0.5617977528089888, 0.48863636363636365, 0.4382022471910112], '特异性_jordan': [0.6814159292035398, 0.7610619469026548, 0.7050147492625368, 0.696165191740413, 0.8017751479289941], '阈值': [0.8768358293587987, 0.8862386723916722, 0.8660223383844489, 0.8852675389679451, 0.876441921150084], '阈值_jordan': [0.8154178664298734, 0.7937188410183781, 0.8130004117041819, 0.8064408167766376, 0.7783350729927102], 'auc': [0.6268270856120115, 0.5877166815816512, 0.6470120314209009, 0.6072003218020916, 0.6272189349112426], 'acc': [0.6565420560747663, 0.6915887850467289, 0.6728971962616822, 0.6510538641686182, 0.7236533957845434], 'precision': [array([0.31875   , 0.85820896]), array([0.32231405, 0.83713355]), array([0.33112583, 0.85920578]), array([0.29251701, 0.83928571]), array([0.36448598, 0.84375   ])], 'recall': [array([0.57303371, 0.67846608]), array([0.43820225, 0.75811209]), array([0.56179775, 0.7020649 ]), array([0.48863636, 0.69321534]), array([0.43820225, 0.79881657])], 'f1': [array([0.40963855, 0.75782537]), array([0.37142857, 0.79566563]), array([0.41666667, 0.77272727]), array([0.36595745, 0.75928918]), array([0.39795918, 0.82066869])], '平均敏感性': 0.752247191011236, '平均特异性': 0.40557330121659596, '平均敏感性_jordan': 0.4999744637385087, '平均特异性_jordan': 0.7290865930076277, '平均阈值': 0.8781612600505898, '平均阈值_jordan': 0.8013826017843563, '平均auc': 0.6191950110655796, 'auc_标准差': 0.020155278885880198, '平均acc': 0.6791470594672678, 'acc_标准差': 0.02636241680245823, '平均precision': 0.5866776861683959, 'precision_标准差': 0.2614394210827424, '平均recall': 0.6130547293819527, 'recall_标准差': 0.1245118259749, '平均f1': 0.5867826569971322, 'f1_标准差': 0.19571388003306908}\n",
      "**cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt****cbt**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"enable_categorical\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "[11:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"enable_categorical\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:01,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "[11:42:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"enable_categorical\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:42:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:01,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "[11:42:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"enable_categorical\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:42:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:02,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 1)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "[11:42:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"enable_categorical\", \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:42:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:03,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 1)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "**xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb**\n",
      "{'敏感性': [0.752808988764045, 0.752808988764045, 0.752808988764045, 0.75, 0.752808988764045], '特异性': [0.39823008849557523, 0.31268436578171094, 0.3421828908554572, 0.36283185840707965, 0.4319526627218935], '敏感性_jordan': [0.7303370786516854, 0.2134831460674157, 0.2808988764044944, 0.6704545454545454, 0.6179775280898876], '特异性_jordan': [0.45132743362831856, 0.9144542772861357, 0.8908554572271387, 0.5191740412979351, 0.5828402366863905], '阈值': [0.9904885, 0.99485695, 0.9931617, 0.9930642, 0.99070144], '阈值_jordan': [0.9883739, 0.8505618, 0.8173863, 0.9850188, 0.984303], 'auc': [0.6050843525239469, 0.5623943521925028, 0.5872195154287229, 0.6134687583802628, 0.616315404560867], 'acc': [0.5070093457943925, 0.7663551401869159, 0.7616822429906542, 0.5480093676814989, 0.5878220140515222], 'precision': [array([0.25793651, 0.86363636]), array([0.3877551 , 0.81530343]), array([0.3968254 , 0.82465753]), array([0.26457399, 0.85784314]), array([0.27918782, 0.85217391])], 'recall': [array([0.73033708, 0.44837758]), array([0.21348315, 0.91150442]), array([0.28089888, 0.8879056 ]), array([0.67045455, 0.51622419]), array([0.61797753, 0.57988166])], 'f1': [array([0.38123167, 0.59029126]), array([0.27536232, 0.86072423]), array([0.32894737, 0.85511364]), array([0.37942122, 0.64456722]), array([0.38461538, 0.69014085])], '平均敏感性': 0.752247191011236, '平均特异性': 0.36957637325234327, '平均敏感性_jordan': 0.5026302349336057, '平均特异性_jordan': 0.6717302892251837, '平均阈值': 0.9924545288085938, '平均阈值_jordan': 0.9251287579536438, '平均auc': 0.5968964766172604, 'auc_标准差': 0.020014078868839016, '平均acc': 0.6341756221409968, 'acc_标准差': 0.10906319780907052, '平均precision': 0.5799893193353469, 'precision_标准差': 0.26667724165116724, '平均recall': 0.5857044630882792, 'recall_标准差': 0.21949694902892294, '平均f1': 0.5390415162002344, 'f1_标准差': 0.20685039504203093}\n",
      "**xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb****xgb**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 37.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(428, 1)\n",
      "(428, 2)\n",
      "(427, 1)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "(427, 1)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "(427, 1)\n",
      "(427, 2)\n",
      "**logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic**\n",
      "{'敏感性': [0.7640449438202247, 0.7752808988764045, 0.7640449438202247, 0.75, 0.7640449438202247], '特异性': [0.3687315634218289, 0.4424778761061947, 0.45427728613569324, 0.45427728613569324, 0.44970414201183434], '敏感性_jordan': [0.6741573033707865, 0.7752808988764045, 0.8202247191011236, 0.7954545454545454, 0.696629213483146], '特异性_jordan': [0.5663716814159292, 0.4424778761061947, 0.41002949852507375, 0.415929203539823, 0.5266272189349113], '阈值': [0.8609798918373959, 0.8379189944121849, 0.838606199087822, 0.8389399294826344, 0.8387516846596997], '阈值_jordan': [0.813505576653425, 0.8379189944121849, 0.8470081751758729, 0.8473166703708506, 0.8240948870267722], 'auc': [0.6064764177521461, 0.6101554472838158, 0.622982334029366, 0.611859747921695, 0.6452031114952463], 'acc': [0.5864485981308412, 0.5093457943925234, 0.4929906542056075, 0.4918032786885246, 0.5597189695550351], 'precision': [array([0.28846154, 0.86818182]), array([0.26640927, 0.8816568 ]), array([0.26642336, 0.8961039 ]), array([0.26022305, 0.88607595]), array([0.27802691, 0.86764706])], 'recall': [array([0.6741573 , 0.56342183]), array([0.7752809 , 0.43952802]), array([0.82022472, 0.40707965]), array([0.79545455, 0.41297935]), array([0.69662921, 0.52366864])], 'f1': [array([0.4040404 , 0.68336315]), array([0.39655172, 0.58661417]), array([0.40220386, 0.55983773]), array([0.39215686, 0.56338028]), array([0.3974359 , 0.65313653])], '平均敏感性': 0.7634831460674157, '平均特异性': 0.43389363076224885, '平均敏感性_jordan': 0.7523493360572011, '平均特异性_jordan': 0.4722870957043863, '平均阈值': 0.8430393398959473, '平均阈值_jordan': 0.8339688607278213, '平均auc': 0.6193354116964539, 'auc_标准差': 0.01405666358801426, '平均acc': 0.5280614589945063, 'acc_标准差': 0.0382260358409076, '平均precision': 0.5759209643901831, 'precision_标准差': 0.30419292809224374, '平均recall': 0.6108424168896783, 'recall_标准差': 0.15368682794885621, '平均f1': 0.5038720608066597, 'f1_标准差': 0.11119620254067729}\n",
      "**logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic****logistic**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vp2yc5FWG04e"
   },
   "source": [
    "# Classification with Keras\n",
    "\n",
    "## Building our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qj3sc2uFEUMt"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomRotation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtools\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodule_util\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_module_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlazy_loader\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLazyLoader\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_LazyLoader\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistribute\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 47\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     48\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_column\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfeature_column_lib\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfeature_column\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;31m# See b/110718070#comment18 for more details about this import.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunctional\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msequential\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtensor_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mlayers\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mlayer_module\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase_layer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[1;31m# Normalization layers.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 174\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormalization\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLayerNormalization\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    175\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormalization_v2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSyncBatchNormalization\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (C:\\Users\\michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-20-9716ada5cf91>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mlayers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSequential\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomRotation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     raise ImportError(\n\u001B[0m\u001B[0;32m      6\u001B[0m         \u001B[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001B[1;31mImportError\u001B[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='gelu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(32, activation='gelu'))\n",
    "model.add(layers.Dense(32, activation='gelu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yrsmpI6EjJ2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "bP0hVm4aElS7",
    "outputId": "aacf234d-d0a9-4de4-91be-5fd45a33b279"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=30,\n",
    "                    batch_size=8,\n",
    "                    callbacks=callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0.005,\n",
    "                        patience=5,\n",
    "                        verbose=0,\n",
    "                        mode='auto',\n",
    "                        baseline=None,\n",
    "                        restore_best_weights=True\n",
    "                    )\n",
    "                   )\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0m1J0_wUFK4C",
    "outputId": "ffd3bf36-29ea-437a-987c-9aa600b9dae6"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f6HrjXeUF0Ko",
    "outputId": "ea282dbd-6f9e-48c7-de2d-dc9afde8949e"
   },
   "outputs": [],
   "source": [
    "print('test_acc: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yQmP_f5Kq0w"
   },
   "source": [
    "Tes accuracy is less than training dataa accuracy. This hints at Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-U2qzRJoHV9O"
   },
   "source": [
    "## Validating our approach\n",
    "Let's set apart 200 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJNbvYZoF7ZT"
   },
   "outputs": [],
   "source": [
    "x_val = X_train[:200]\n",
    "partial_x_train = X_train[200:]\n",
    "\n",
    "y_val = y_train[:200]\n",
    "partial_y_train = y_train[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1EkG59EHeEV"
   },
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "Dp3G4P3aP4k2",
    "outputId": "25e1a389-1ac2-425b-bd5f-05736b6e9b96"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=30,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dljqHfDPI6lH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mvi9it1SI4aR",
    "outputId": "98b01ef2-3935-442b-82d6-45f56e036d39"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3hb8s1l4rBA"
   },
   "source": [
    "## Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gudBAhIXJIi2"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xb7bVPSwJQF0",
    "outputId": "aca09c75-1d21-4847-bdd9-a0521dc8d948"
   },
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "llusRQV0JRy9",
    "outputId": "a856289d-883a-47cb-c0fb-ec148330a60a"
   },
   "outputs": [],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0eoEuSZqJTdU",
    "outputId": "94c17d00-dd7f-40a1-84d2-78d1ebde6103"
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Utgt1bXfJVRN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "name": "Untitled9.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}